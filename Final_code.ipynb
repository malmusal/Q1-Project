{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README:\n",
    "To run this code, you need to do the following: \\\n",
    "Python \\\n",
    "PyTorch \\\n",
    "PyTorch Geometric (PyG) \\\n",
    "scikit-learn \\\n",
    "Matplotlib \\\n",
    "tqdm \n",
    "\n",
    "You can install the required packages using pip. For PyTorch and PyTorch Geometric, follow the instructions provided in their respective installation guides, as their installation commands can vary depending on your system's CUDA version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from typing import Any, Dict, Optional\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, ReLU, GRU, Embedding, LSTM, Dropout, BatchNorm1d, BatchNorm2d, ModuleList, Module\n",
    "from torch_geometric.nn import GCNConv, GINConv, GATv2Conv, GINEConv, GPSConv,global_mean_pool, TopKPooling\n",
    "from torch_geometric.datasets import Planetoid, TUDataset, LRGBDataset \n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    "    extract_zip,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch_geometric.io import fs\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import pickle\n",
    "from typing import Callable, List, Optional\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GCN_graph(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 64)\n",
    "        self.conv2 = GCNConv(64, 64)\n",
    "        self.conv3 = GCNConv(64, 64)  \n",
    "        self.conv4 = GCNConv(64, 64)\n",
    "        self.fc = torch.nn.Linear(64, num_classes)  \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = data.x.float()\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5,training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.dropout(x, p=0.5,training=self.training)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5,training=self.training)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# class GCN_graph(torch.nn.Module):\n",
    "#     def __init__(self, num_node_features, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = GCNConv(num_node_features, 64)\n",
    "#         self.conv2 = GCNConv(64, 64)\n",
    "#         self.conv3 = GCNConv(64, 64)  \n",
    "#         self.conv4 = GCNConv(64, 64)\n",
    "#         self.fc = torch.nn.Linear(64, num_classes)  \n",
    "\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = F.relu(self.conv3(x, edge_index))\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = F.relu(self.conv4(x, edge_index))\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "#         # Apply global mean pooling to aggregate node features into graph-level features\n",
    "#         x = global_mean_pool(x, batch)\n",
    "\n",
    "#         x = self.fc(x)\n",
    "\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(num_node_features, 8, heads=8, dropout=0.6)\n",
    "        # On the Pubmed dataset, we've seen that using too many heads can lead to overfitting, so we'll use 8 heads.\n",
    "        self.conv2 = GATv2Conv(8 * 8, num_classes, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class GATv2_Graph(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GATv2_Graph, self).__init__()\n",
    "        self.conv1 = GATv2Conv(num_node_features, 32, heads=8, concat=True, dropout=0.0)\n",
    "        ## add two more convultions\n",
    "        self.conv2 = GATv2Conv(32*8, 32, heads=8, concat=True, dropout=0.0)\n",
    "        self.conv3 = GATv2Conv(32*8, 32, heads=1, concat=True, dropout=0.0)\n",
    "        self.fc1 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = data.x.float()\n",
    "        \n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        \n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        \n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "\n",
    "        # Global Mean Pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class GIN_Node(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GIN_Node, self).__init__()\n",
    "        \n",
    "        \n",
    "        nn1 = Sequential(\n",
    "            Linear(num_node_features, 64),\n",
    "            ReLU(),\n",
    "            Linear(64, 64)\n",
    "        )\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = BatchNorm1d(64)\n",
    "\n",
    "        # Define the second GINConv layer using Sequential\n",
    "        nn2 = Sequential(\n",
    "            Linear(64, 64),\n",
    "            ReLU(),\n",
    "            Linear(64, 64)\n",
    "        )\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.bn2 = BatchNorm1d(64)\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = Linear(64, 64)\n",
    "        self.fc2 = Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = data.x.float()\n",
    "\n",
    "        # Apply GNN layers\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "\n",
    "class GIN_Graph(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GIN_Graph, self).__init__()\n",
    "        \n",
    "        \n",
    "        nn1 = Sequential(\n",
    "            Linear(num_node_features, 64),\n",
    "            ReLU(),\n",
    "            Linear(64, 64)\n",
    "        )\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = BatchNorm1d(64)\n",
    "\n",
    "        # Define the second GINConv layer using Sequential\n",
    "        nn2 = Sequential(\n",
    "            Linear(64, 64),\n",
    "            ReLU(),\n",
    "            Linear(64, 64)\n",
    "        )\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.bn2 = BatchNorm1d(64)\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = Linear(64, 64)\n",
    "        self.fc2 = Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)  \n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Global mean pooling\n",
    "        x = global_mean_pool(x, batch)  \n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class GPS_Node(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, dropout_rate=0.5):\n",
    "        super(GPS_Node, self).__init__()\n",
    "        self.preprocess = torch.nn.Linear(num_node_features, 64)\n",
    "        self.conv = GPSConv(64, GCNConv(64, 64), dropout=0.5, heads=4)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate) \n",
    "        self.fc = torch.nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = self.preprocess(x)\n",
    "        x = F.relu(x)  \n",
    "        x = self.dropout(x)  \n",
    "\n",
    "        x = self.conv(x, edge_index)\n",
    "        x = F.relu(x)  \n",
    "        x = self.dropout(x)  \n",
    "\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GPS_Graph(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, dropout_rate=0.5):\n",
    "        super(GPS_Graph, self).__init__()\n",
    "        self.preprocess = torch.nn.Linear(num_node_features, 64)\n",
    "        self.conv = GPSConv(64, GCNConv(64, 64), dropout=0.5, heads=4)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate) \n",
    "        self.fc1 = torch.nn.Linear(64, 64)  \n",
    "        self.fc2 = torch.nn.Linear(64, num_classes)  \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = data.x.float()\n",
    "        x = self.preprocess(x)\n",
    "        x = F.relu(x)  \n",
    "        x = self.dropout(x)  \n",
    "\n",
    "        x = self.conv(x, edge_index)\n",
    "        x = F.relu(x)  \n",
    "        x = self.dropout(x)  \n",
    "\n",
    "        x = global_mean_pool(x, batch)  \n",
    "\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = self.dropout(x)  \n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch.to(device)\n",
    "            outputs = model(batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            if batch.y.ndim > 1:\n",
    "                labels = batch.y.argmax(dim=1)\n",
    "            else:\n",
    "                labels = batch.y\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CORA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(dataset.num_features, dataset.num_classes).to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)  # Predictions for all nodes\n",
    "train_correct = (pred[data.train_mask] == data.y[data.train_mask]).sum().item()  \n",
    "train_acc = train_correct / data.train_mask.sum().item()  \n",
    "\n",
    "# Evaluation for test data\n",
    "test_correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()  \n",
    "test_acc = test_correct / data.test_mask.sum().item()  \n",
    "\n",
    "#\n",
    "cora_GCN = {\n",
    "    'train_accuracy': train_acc,\n",
    "    'test_accuracy': test_acc\n",
    "}\n",
    "cora_GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMBD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = TUDataset(root='/tmp/IMDB', name='IMDB-BINARY')\n",
    "\n",
    "# Create a new list to store modified graphs\n",
    "modified_graphs = []\n",
    "\n",
    "# Add features if not present\n",
    "for data in imdb_dataset:\n",
    "    if data.x is None:\n",
    "        # Add a dummy feature (e.g., all ones)\n",
    "        num_nodes = data.num_nodes\n",
    "        one_features = torch.ones((num_nodes, 3))\n",
    "        data.x = one_features\n",
    "    modified_graphs.append(data)\n",
    "\n",
    "\n",
    "\n",
    "random.shuffle(modified_graphs)\n",
    "\n",
    "split_idx = int(len(modified_graphs) * 0.8)\n",
    "train_dataset = modified_graphs[:split_idx]\n",
    "test_dataset = modified_graphs[split_idx:]\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = GCN_graph(num_node_features=3, num_classes=2)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "for epoch in range(400):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = F.nll_loss(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # print(f\"Epoch {epoch} | Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_accuracy = evaluate(train_loader)\n",
    "test_accuracy = evaluate(test_loader)\n",
    "\n",
    "\n",
    "accuracies = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "print(f\"Accuracies: {accuracies}\")\n",
    "imdb_acc_GCN = accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENZYME:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enzymes_dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(enzymes_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model instantiation\n",
    "model = GCN_graph(num_node_features=3, num_classes=6)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(400):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = F.nll_loss(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #print(f\"Epoch {epoch} | Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_accuracy = evaluate(train_loader)\n",
    "test_accuracy = evaluate(test_loader)\n",
    "\n",
    "\n",
    "accuracies = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "print(f\"Accuracies: {accuracies}\")\n",
    "enzyme_acc_GCN = accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LRGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pep_train = LRGBDataset(root='path/to/data', name='Peptides-func', split='train')\n",
    "pep_test = LRGBDataset(root='path/to/data', name='Peptides-func', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(pep_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(pep_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model instantiation\n",
    "model = GCN_graph(9, 10)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#Train the model\n",
    "\n",
    "for epoch in range(400):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        if batch.y.ndim > 1:\n",
    "            target = batch.y.argmax(dim=1)\n",
    "        else:\n",
    "            target = batch.y\n",
    "\n",
    "        loss = F.nll_loss(out, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch%50 == 0:\n",
    "        print(f\"Epoch {epoch} | Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_accuracy = evaluate(train_loader)\n",
    "test_accuracy = evaluate(test_loader)\n",
    "\n",
    "# Save the accuracies in a dictionary\n",
    "accuracies = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "print(f\"Accuracies: {accuracies}\")\n",
    "pascal_acc_GCN = accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GIN_Node(dataset.num_features, dataset.num_classes).to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)  # Predictions for all nodes\n",
    "train_correct = (pred[data.train_mask] == data.y[data.train_mask]).sum().item()  \n",
    "train_acc = train_correct / data.train_mask.sum().item()  \n",
    "\n",
    "# Evaluation for test data\n",
    "test_correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()  \n",
    "test_acc = test_correct / data.test_mask.sum().item()  \n",
    "\n",
    "#\n",
    "cora_GIN = {\n",
    "    'train_accuracy': train_acc,\n",
    "    'test_accuracy': test_acc\n",
    "}\n",
    "cora_GIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = TUDataset(root='/tmp/IMDB', name='IMDB-BINARY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_graphs = []\n",
    "\n",
    "\n",
    "for data in imdb_dataset:\n",
    "    if data.x is None:\n",
    "        \n",
    "        num_nodes = data.num_nodes\n",
    "        one_features = torch.ones((num_nodes, 3))\n",
    "        data.x = one_features\n",
    "    modified_graphs.append(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random.shuffle(modified_graphs)\n",
    "\n",
    "split_idx = int(len(modified_graphs) * 0.8)\n",
    "train_dataset = modified_graphs[:split_idx]\n",
    "test_dataset = modified_graphs[split_idx:]\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = GIN_Graph(3, 2)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(400):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = F.nll_loss(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_accuracy = evaluate(train_loader)\n",
    "test_accuracy = evaluate(test_loader)\n",
    "\n",
    "# Save the accuracies in a dictionary\n",
    "accuracies = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "imdb_acc_GIN = accuracies\n",
    "\n",
    "print(f\"Accuracies: {accuracies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENZYME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enzyme_dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(enzyme_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model instantiation\n",
    "model = GIN_Graph(3, 6)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(400):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = F.nll_loss(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_accuracy = evaluate(train_loader)\n",
    "test_accuracy = evaluate(test_loader)\n",
    "\n",
    "# Save the accuracies in a dictionary\n",
    "accuracies = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "print(f\"Accuracies: {accuracies}\")\n",
    "enzyme_acc_GIN = accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pep_train = LRGBDataset(root='path/to/data', name='Peptides-func', split='train')\n",
    "pep_test = LRGBDataset(root='path/to/data', name='Peptides-func', split='test')\n",
    "\n",
    "train_loader = DataLoader(pep_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(pep_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model instantiation\n",
    "model = GIN_Graph(9, 10)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#Train the model\n",
    "for epoch in range(400):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        if batch.y.ndim > 1:\n",
    "            target = batch.y.argmax(dim=1)\n",
    "        else:\n",
    "            target = batch.y\n",
    "\n",
    "        loss = F.nll_loss(out, target)\n",
    "\n",
    "        # loss = F.nll_loss(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch%50 == 0:\n",
    "        print(f\"Epoch {epoch} | Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_accuracy = evaluate(train_loader)\n",
    "test_accuracy = evaluate(test_loader)\n",
    "\n",
    "# Save the accuracies in a dictionary\n",
    "accuracies = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "print(f\"Accuracies: {accuracies}\")\n",
    "pascal_acc_GIN = accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GATv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GATv2(dataset.num_node_features, dataset.num_classes).to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)  # Predictions for all nodes\n",
    "train_correct = (pred[data.train_mask] == data.y[data.train_mask]).sum().item()  \n",
    "train_acc = train_correct / data.train_mask.sum().item()\n",
    "\n",
    "\n",
    "# Evaluation for test data\n",
    "test_correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()  \n",
    "test_acc = test_correct / data.test_mask.sum().item()  \n",
    "\n",
    "#\n",
    "cora_GAT = {\n",
    "    'train_accuracy': train_acc,\n",
    "    'test_accuracy': test_acc\n",
    "}\n",
    "cora_GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_graphs = []\n",
    "\n",
    "# Add features if not present\n",
    "for data in imdb_dataset:\n",
    "    if data.x is None:\n",
    "        # Add a dummy feature (e.g., all ones)\n",
    "        num_nodes = data.num_nodes\n",
    "        one_features = torch.ones((num_nodes, 3))\n",
    "        data.x = one_features\n",
    "    modified_graphs.append(data)\n",
    "\n",
    "random.shuffle(modified_graphs)\n",
    "\n",
    "split_idx = int(len(modified_graphs) * 0.8)\n",
    "train_dataset = modified_graphs[:split_idx]\n",
    "test_dataset = modified_graphs[split_idx:]\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = GATv2_Graph(num_node_features=3, num_classes=2)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(400):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = F.nll_loss(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_accuracy = evaluate(train_loader)\n",
    "test_accuracy = evaluate(test_loader)\n",
    "\n",
    "# Save the accuracies in a dictionary\n",
    "accuracies = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "imdb_acc_GAT = accuracies\n",
    "\n",
    "print(f\"Accuracies: {accuracies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENZYME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(enzyme_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model instantiation\n",
    "model = GATv2_Graph(num_node_features=3, num_classes=6)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(400):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = F.nll_loss(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #print(f\"Epoch {epoch} | Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_accuracy = evaluate(train_loader)\n",
    "test_accuracy = evaluate(test_loader)\n",
    "\n",
    "# Save the accuracies in a dictionary\n",
    "accuracies = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "enzyme_acc_GAT = accuracies\n",
    "print(f\"Accuracies: {accuracies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LRGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(pep_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(pep_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model instantiation\n",
    "model = GATv2_Graph(num_node_features=9, num_classes=10)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#Train the model\n",
    "for epoch in range(400):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        if batch.y.ndim > 1:\n",
    "            target = batch.y.argmax(dim=1)\n",
    "        else:\n",
    "            target = batch.y\n",
    "\n",
    "        loss = F.nll_loss(out, target)\n",
    "\n",
    "        # loss = F.nll_loss(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch%50 == 0:\n",
    "        print(f\"Epoch {epoch} | Loss: {loss.item()}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_accuracy = evaluate(train_loader)\n",
    "test_accuracy = evaluate(test_loader)\n",
    "\n",
    "# Save the accuracies in a dictionary\n",
    "accuracies = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "print(f\"Accuracies: {accuracies}\")\n",
    "pascal_acc_GAT = accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = '/tmp/Cora/Cora/' \n",
    "processed_path = os.path.join(dataset_path, 'processed')\n",
    "\n",
    "# Check if the processed directory exists\n",
    "if os.path.exists(processed_path):\n",
    "    print(\"Dataset is already loaded. Deleting processed files...\")\n",
    "    shutil.rmtree(processed_path)\n",
    "    print(\"Processed files deleted.\")\n",
    "else:\n",
    "    print(\"Dataset not found or not loaded yet.\")\n",
    "\n",
    "transform = T.AddRandomWalkPE(10)\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora', pre_transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPS_Node(dataset.num_node_features, dataset.num_classes).to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "train_correct = (pred[data.train_mask] == data.y[data.train_mask]).sum().item()  \n",
    "train_acc = train_correct / data.train_mask.sum().item()  \n",
    "\n",
    "# Evaluation for test data\n",
    "test_correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()  \n",
    "test_acc = test_correct / data.test_mask.sum().item()  \n",
    "\n",
    "#\n",
    "accuracy = {\n",
    "    'train_accuracy': train_acc,\n",
    "    'test_accuracy': test_acc\n",
    "}\n",
    "\n",
    "cora_GPS = accuracy\n",
    "cora_GPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/tmp/IMDB/IMDB-BINARY/'\n",
    "processed_path = os.path.join(dataset_path, 'processed')\n",
    "\n",
    "# Check if the processed directory exists\n",
    "if os.path.exists(processed_path):\n",
    "    print(\"Dataset is already loaded. Deleting processed files...\")\n",
    "    shutil.rmtree(processed_path)\n",
    "    print(\"Processed files deleted.\")\n",
    "else:\n",
    "    print(\"Dataset not found or not loaded yet.\")\n",
    "\n",
    "transform = T.AddRandomWalkPE(10)\n",
    "\n",
    "imdb_dataset = TUDataset(root='/tmp/IMDB', name='IMDB-BINARY', pre_transform=transform)\n",
    "modified_graphs = []\n",
    "\n",
    "for data in imdb_dataset:\n",
    "    if data.x is None:\n",
    "        \n",
    "        num_nodes = data.num_nodes\n",
    "        one_features = torch.ones((num_nodes, 3))\n",
    "        data.x = one_features\n",
    "    modified_graphs.append(data)\n",
    "\n",
    "random.shuffle(modified_graphs)\n",
    "\n",
    "split_idx = int(len(modified_graphs) * 0.8)\n",
    "train_dataset = modified_graphs[:split_idx]\n",
    "test_dataset = modified_graphs[split_idx:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPS_Graph(3, 2)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(400):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = F.nll_loss(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #print(f\"Epoch {epoch} | Loss: {loss.item()}\")\n",
    "    \n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_accuracy = evaluate(train_loader)\n",
    "test_accuracy = evaluate(test_loader)\n",
    "\n",
    "\n",
    "accuracies = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "imdb_acc_GPS = accuracies\n",
    "\n",
    "print(f\"Accuracies: {accuracies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENZYME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/tmp/ENZYMES/ENZYMES'\n",
    "processed_path = os.path.join(dataset_path, 'processed')\n",
    "\n",
    "# Check if the processed directory exists\n",
    "if os.path.exists(processed_path):\n",
    "    print(\"Dataset is already loaded. Deleting processed files...\")\n",
    "    shutil.rmtree(processed_path)\n",
    "    print(\"Processed files deleted.\")\n",
    "else:\n",
    "    print(\"Dataset not found or not loaded yet.\")\n",
    "\n",
    "transform = T.AddRandomWalkPE(10)\n",
    "\n",
    "enzyme_dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES', pre_transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(enzyme_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPS_Graph(3, 6)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(400):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = F.nll_loss(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_accuracy = evaluate(train_loader)\n",
    "test_accuracy = evaluate(test_loader)\n",
    "\n",
    "\n",
    "accuracies = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "enzyme_acc_GPS = accuracies\n",
    "\n",
    "print(f\"Accuracies: {accuracies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'path/to/data/peptides-func/'\n",
    "processed_path = os.path.join(dataset_path, 'processed')\n",
    "\n",
    "# Check if the processed directory exists\n",
    "if os.path.exists(processed_path):\n",
    "    print(\"Dataset is already loaded. Deleting processed files...\")\n",
    "    shutil.rmtree(processed_path)\n",
    "    print(\"Processed files deleted.\")\n",
    "else:\n",
    "\n",
    "    print(\"Dataset not found or not loaded yet.\")\n",
    "\n",
    "transform = T.AddRandomWalkPE(10)\n",
    "\n",
    "pep_train = LRGBDataset(root='path/to/data', name='Peptides-func', split='train', pre_transform=transform)\n",
    "pep_test = LRGBDataset(root='path/to/data', name='Peptides-func', split='test', pre_transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(pep_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(pep_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model instantiation\n",
    "model = GPS_Graph(9, 10)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#Train the model\n",
    "for epoch in range(400):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        if batch.y.ndim > 1:\n",
    "            target = batch.y.argmax(dim=1)\n",
    "        else:\n",
    "            target = batch.y\n",
    "\n",
    "        loss = F.nll_loss(out, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #print(f\"Epoch {epoch} | Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_accuracy = evaluate(train_loader)\n",
    "test_accuracy = evaluate(test_loader)\n",
    "\n",
    "# Save the accuracies in a dictionary\n",
    "accuracies = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "print(f\"Accuracies: {accuracies}\")\n",
    "pascal_acc_GPS = accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
